import streamlit as st
import hashlib
import numpy as np
import time
import threading
import requests
import json
from random import choice
from transformers import pipeline
from scipy.stats import ks_2samp

try:
    import sounddevice as sd
    import speech_recognition as sr
    has_audio = True
except Exception:
    has_audio = False

try:
    import websocket
    has_ws = True
except ImportError:
    has_ws = False

st.set_page_config(page_title="IntelliCore AGI", layout="wide")
PASSWORD = "Stakeholder2025"

def check_password():
    def encrypt(p): return hashlib.sha256(p.encode()).hexdigest()
    input_pass = st.sidebar.text_input("ğŸ” Enter Password:", type="password")
    if encrypt(input_pass) != encrypt(PASSWORD):
        st.warning("ğŸ”’ Access denied")
        st.stop()

check_password()
st.sidebar.success("âœ… Access Granted")

st.title("ğŸ¤– IntelliCore AGI â€” Unified Cognitive Control System")
st.video("https://intellicore.ai/assets/demo_walkthrough.mp4")
st.markdown("Explore IntelliCoreâ€™s full AGI stack: voice, reasoning, agents, telemetry, and more.")

if 'last_decision' not in st.session_state:
    st.session_state['last_decision'] = None

tabs = st.tabs([
    "ğŸŒ Ask IntelliCore", "ğŸ›° Agents", "ğŸ“¡ Telemetry",
    "ğŸ”„ Reflection", "ğŸ¤ Voice", "ğŸ˜Š Emotion", "âš ï¸ Drift"
])

# ğŸŒ Ask IntelliCore (Cortex)
with tabs[0]:
    st.markdown("### ğŸ§  Ask IntelliCore Cortex")
    question = st.text_input("What would you like to ask the system?")
    if st.button("ğŸ§  Generate Response"):
        st.session_state['last_decision'] = choice([
            "Initiating strategic scan of Zone C.",
            "All systems are currently optimal.",
            "Power conservation enabled across northern agents."
        ])
        st.success("Cortex has responded.")
    if st.session_state['last_decision']:
        st.info(f"ğŸ¤– Cortex: {st.session_state['last_decision']}")
        if st.button("ğŸš€ Execute Decision"):
            st.success("ğŸ›° Decision Executed.")

# ğŸ›° Agent Command Center
with tabs[1]:
    st.markdown("### ğŸ›° Agent Operations")
    c1, c2, c3 = st.columns(3)
    if c1.button("Deploy Drone"):
        st.info("ğŸ›¸ Drone deployed to Sector A.")
    if c2.button("Activate Humanoid"):
        st.info("ğŸ¤– Humanoid operational in MedBay.")
    if c3.button("Contact Virtual Agent"):
        st.success("ğŸ’¬ Virtual agent engaging...")

# ğŸ“¡ Telemetry (WebSocket or mock)
with tabs[2]:
    st.markdown("### ğŸ“¡ Live Agent Telemetry")
    telemetry_box = st.empty()

    def mock_stream():
        updates = [
            {"agent": "drone", "status": "Scanning", "zone": "Sector A"},
            {"agent": "humanoid", "status": "Assisting", "zone": "Zone B"},
            {"agent": "virtual", "status": "Reporting", "zone": "HQ"}
        ]
        for _ in range(10):
            telemetry_box.json(choice(updates))
            time.sleep(1)

    def start_websocket_stream():
        def on_message(ws, message):
            data = json.loads(message)
            telemetry_box.json(data)
        ws = websocket.WebSocketApp("wss://your-backend.example/ws/telemetry", on_message=on_message)
        threading.Thread(target=ws.run_forever).start()

    if has_ws and st.button("ğŸ“¶ Start WebSocket Feed"):
        start_websocket_stream()
    if st.button("â–¶ï¸ Use Simulated Telemetry"):
        threading.Thread(target=mock_stream).start()

# ğŸ”„ Self-Reflection
with tabs[3]:
    st.markdown("### ğŸ”„ Self-Reflection Logs")
    logs = [
        {"timestamp": "2025-04-15T12:01Z", "change": "Reduced redundant scans", "why": "Battery preservation"},
        {"timestamp": "2025-04-14T09:22Z", "change": "Switched from GPS to vision nav", "why": "Improved accuracy"}
    ]
    for log in logs:
        st.markdown(f"**ğŸ•’ {log['timestamp']}** â€” *{log['change']}*  
> _Reason:_ {log['why']}")

# ğŸ¤ Voice to Cortex
with tabs[4]:
    st.markdown("### ğŸ¤ Voice Input")
    if not has_audio:
        st.warning("ğŸ”‡ Voice input not supported in this environment.")
    else:
        if st.button("ğŸ§ Start Listening"):
            recognizer = sr.Recognizer()
            mic = sr.Microphone()
            with mic as source:
                st.info("Listening for 5 seconds...")
                audio = recognizer.listen(source, timeout=5)
            try:
                result = recognizer.recognize_google(audio)
                st.success(f"ğŸ—£ You said: {result}")
                response = choice([
                    "Voice acknowledged. Routing signal to cortex.",
                    "Analyzing acoustic command. Executingâ€¦",
                    "Drone engagement authorized via voice link."
                ])
                st.info(f"ğŸ¤– Cortex: {response}")
            except Exception as e:
                st.error(f"Speech recognition error: {e}")

# ğŸ˜Š Emotion NLP
with tabs[5]:
    st.markdown("### ğŸ˜Š Emotion Analysis")
    text = st.text_area("Input text for emotional analysis:")
    if st.button("ğŸ” Analyze Emotion"):
        emo = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")
        st.json(emo(text))

# âš ï¸ Drift Detection
with tabs[6]:
    st.markdown("### âš ï¸ Drift Detection")
    if st.button("ğŸ“Š Run Drift Test"):
        ref = np.random.normal(size=500)
        new = np.concatenate([ref, np.random.normal(loc=1.2, size=100)])
        stat, p = ks_2samp(ref, new)
        st.metric("Drift?", "Yes" if p < 0.05 else "No")
        st.metric("p-value", round(p, 4))
