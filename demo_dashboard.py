import streamlit as st
import hashlib
import numpy as np
import time
import threading
import requests
from random import choice
from transformers import pipeline
from scipy.stats import ks_2samp

# Safe audio import
try:
    import sounddevice as sd
    import speech_recognition as sr
    has_audio = True
except OSError:
    has_audio = False

# Page setup
st.set_page_config(page_title="IntelliCore AGI", layout="wide")
PASSWORD = "Stakeholder2025"
API_URL = "https://demo.intellicore.ai"

# Password gate
def check_password():
    def encrypt(p): return hashlib.sha256(p.encode()).hexdigest()
    input_pass = st.sidebar.text_input("ğŸ” Enter Password:", type="password")
    if encrypt(input_pass) != encrypt(PASSWORD):
        st.warning("ğŸ”’ Access denied")
        st.stop()

check_password()
st.sidebar.success("âœ… Access Granted")

# Branding
st.image("https://intellicore.ai/assets/logo_dark.png", width=140)
st.title("ğŸ¤– IntelliCore AGI â€” Unified AGI Control & Demo")

if 'last_decision' not in st.session_state:
    st.session_state['last_decision'] = None

# Define all tabs
tabs = st.tabs([
    "ğŸŒ Home", "ğŸ›° Agent Control", "ğŸ“¡ Telemetry",
    "ğŸ”„ Reflection", "ğŸ¤ Voice", "ğŸ˜Š Emotion", "âš ï¸ Drift"
])

# ğŸŒ Home Tab
with tabs[0]:
    st.markdown("### ğŸ§  Ask IntelliCore")
    question = st.text_input("Prompt", placeholder="e.g. Should we initiate a tactical scan of Zone C?")
    if st.button("ğŸ§  Generate Decision"):
        st.session_state['last_decision'] = choice([
            "Deploy drone to Zone C for tactical scan.",
            "Delay operation due to low visibility.",
            "Initiate satellite imaging of Region 7."
        ])
        st.success("Cortex has responded.")
    if st.session_state['last_decision']:
        st.info(f"**ğŸ¤– Decision:** {st.session_state['last_decision']}")
        if st.button("ğŸš€ Execute"):
            st.success("ğŸ›° Executing decision logicâ€¦")

# ğŸ›° Agent Control
with tabs[1]:
    st.markdown("### ğŸ›° Agent Command Center")
    c1, c2, c3 = st.columns(3)
    if c1.button("Deploy Drone"):
        st.info("ğŸ›° Drone launched to Sector A.")
    if c2.button("Activate Humanoid"):
        st.info("ğŸ§ Humanoid operational in MedBay.")
    if c3.button("Contact Virtual Agent"):
        st.success("ğŸ’¬ Virtual Agent is online.")

# ğŸ“¡ Telemetry
with tabs[2]:
    st.markdown("### ğŸ“¡ Live Agent Telemetry")
    telemetry_box = st.empty()
    if st.button("ğŸ“¶ Start Feed"):
        def stream():
            updates = [
                {"agent": "drone", "status": "Scanning", "zone": "Sector A"},
                {"agent": "humanoid", "status": "Assisting", "zone": "Zone B"},
                {"agent": "virtual", "status": "Reporting", "zone": "HQ"}
            ]
            for _ in range(10):
                telemetry_box.json(choice(updates))
                time.sleep(1)
        threading.Thread(target=stream).start()

# ğŸ”„ Reflection
with tabs[3]:
    st.markdown("### ğŸ”„ Self-Reflection Logs")
    logs = [
        {"timestamp": "2025-04-15T12:01Z", "change": "Reduced redundant scans", "why": "Battery preservation"},
        {"timestamp": "2025-04-14T09:22Z", "change": "Switched from GPS to vision nav", "why": "Improved accuracy"}
    ]
    for log in logs:
        st.markdown(f"**ğŸ•’ {log['timestamp']}** â€” *{log['change']}*  \n> _Reason:_ {log['why']}")

# ğŸ¤ Voice
with tabs[4]:
    st.markdown("### ğŸ¤ Voice Transcription")
    if not has_audio:
        st.warning("ğŸ”‡ Audio recording not supported in this environment.")
        mock_input = st.text_input("Manual transcription (mock):")
        if mock_input:
            st.success(f"ğŸ—£ You said: {mock_input}")
    else:
        if st.button("ğŸ™ Start Recording"):
            recognizer = sr.Recognizer()
            with sd.InputStream(samplerate=16000, channels=1) as stream:
                st.info("Listening for 5s...")
                audio = stream.read(16000 * 5)[0]
            audio_data = sr.AudioData(np.array(audio).tobytes(), 16000, 2)
            try:
                result = recognizer.recognize_google(audio_data)
                st.success(f"ğŸ”Š {result}")
            except Exception as e:
                st.error(f"Speech recognition error: {e}")

# ğŸ˜Š Emotion
with tabs[5]:
    st.markdown("### ğŸ˜Š Emotion Analysis")
    text = st.text_area("Type input for analysis:")
    if st.button("ğŸ” Analyze"):
        emo = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")
        st.json(emo(text))

# âš ï¸ Drift
with tabs[6]:
    st.markdown("### âš ï¸ Data Drift Detection")
    if st.button("ğŸ” Run Drift Test"):
        ref = np.random.normal(size=500)
        new = np.concatenate([ref, np.random.normal(loc=1.3, size=100)])
        stat, p = ks_2samp(ref, new)
        st.metric("Drift Detected", "Yes" if p < 0.05 else "No")
        st.metric("p-value", round(p, 4))
